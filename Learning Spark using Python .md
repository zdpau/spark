使用hadoop能够得到我无法用SQL查询看到的结果。当我工作时，我必须写程序。在合并这两个数据集，然后过滤，然后合并另一个数据集，然后使用一些条件过滤。你必须写超过3个Mapreduce任务。一份工作将逐行读取数据并写入磁盘。有大量的数据运动在这两者之间发生了进一步的影响速度。

另一件我讨厌的事情是，没有直接的方法将文件传递给mappers和reducers，这通常会在整个序列中添加另一个mapreduce作业。这只是程序性的任务。即使在完成并行的整个逻辑之后，实现迭代算法也是一个挑战。将会有很多mapreduce任务，一个基于shell的驱动程序，以及许多独特的想法，将所有的东西结合在一起。

这让我想到了Hadoop的整个实现方式。当Hadoop出现时，RAM很昂贵。但事实并非如此。我们的Hadoop集群中已经有64GB的机器了。因此，不要使用更大的内存块并逐行读取，这真的是个好主意。我们也可以有一些东西让我们在内存中保留一个特定的数据块，这样下次我们的程序需要它的时候就不必再读它，浪费时间了。如果我们有一些变量，让我们保持迭代算法的状态，那就更好了。

但使用spark之后，你仍然不得不用mapreduce的方式思考，但是map和reduce步骤完成的方式有些不同。
